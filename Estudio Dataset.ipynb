{"cells":[{"cell_type":"markdown","metadata":{"id":"8iI5I2qn9t-n"},"source":["# Mean and STD of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"l8wgRoyCdqS9","outputId":"b320737c-f70e-421a-ab9a-5d0c491537ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting calculation\n","Analysis of batch  1\n","Analysis of batch  2\n","Analysis of batch  3\n","Analysis of batch  4\n","Analysis of batch  5\n","Analysis of batch  6\n","Analysis of batch  7\n","Analysis of batch  8\n","Analysis of batch  9\n","Analysis of batch  10\n","Analysis of batch  11\n","tensor([0.7649, 0.5422, 0.5683])\n","tensor([0.1376, 0.1590, 0.1754])\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, datasets\n","\n","def statisticalAnalysis(dataloader):\n","    print(\"Starting calculation\")\n","    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n","    for data, _ in dataloader:\n","        print(\"Analysis of batch \", 1 + num_batches)\n","\n","        channels_sum += torch.mean(data, dim=[0,2,3])\n","        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n","        num_batches += 1\n","     \n","    mean = channels_sum / num_batches\n","    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n","    print(mean)\n","    print (std)\n","\n","DATASET_NAME = \"224\"\n","MAIN_DATA_DIR = '/content/drive/My Drive/Master/TFM/'\n","TRAIN_DIR = MAIN_DATA_DIR + DATASET_NAME\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","imageDataset = datasets.ImageFolder(TRAIN_DIR, transform = transform)\n","dataLoader = DataLoader(imageDataset,  batch_size=1000, shuffle=False, num_workers=1)\n","statisticalAnalysis(dataLoader)"]},{"cell_type":"markdown","metadata":{"id":"JtLWPF9p9np7"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNbDzS6f9xUf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FlR0RoaCc4Dz"},"source":["# Crear datasets cuadrados\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quYEMalLc088"},"outputs":[],"source":["\n","\n","workingDir = \"drive/Master/TFM/64_min\"\n","metadata = pd.read_csv('/content/drive/My Drive/Master/TFM/64/HAM10000_metadata.csv')\n","basedir = \"/content/drive/My Drive/Master/TFM/\"\n","\n","le = LabelEncoder()\n","le.fit(metadata['dx'])\n","LabelEncoder()\n","label = list(le.classes_)\n","label_images = []\n","\n","print(\"Classes:\", label)\n"," \n","metadata['label'] = le.transform(metadata[\"dx\"]) \n","\n","\n","def sortImages(data_dir):\n","    print(\"Current folder:\" + data_dir)\n","    for i in label:\n","        label_images = []\n","        try:\n","            os.mkdir(data_dir + \"/\" + str(i) + \"/\")\n","            print(i)\n","        except:\n","            print(\"\")\n","            \n","        print(\"Processing group r: \" +  str(i) + \"from dataset\" + data_dir)\n","        sample = metadata[metadata['dx'] == i]['image_id']\n","        label_images.extend(sample)\n","        \n","        for id in label_images:\n","            shutil.copyfile((data_dir + \"/\"+ id +\".png\"), (data_dir + \"/\" + i + \"/\"+id+\".png\"))\n","  \n","\n","\n","#sortImages(basedir  + \"/\" + str(128))\n","#resolutionsCenterCrop = [64,96,128,192,224]\n","#resolutionsCenterCrop = [96,192,224]\n","\n","resolutionsCenterCrop = [64]\n","\n","for folder in resolutionsCenterCrop:\n","    outputDir = basedir  + \"/\" + str(folder)\n","    sortImages(outputDir)"]},{"cell_type":"markdown","metadata":{"id":"cErDgIvhdHMt"},"source":["# AnÃ¡lisis Media y STD\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L2cn-rhdMZp"},"outputs":[],"source":["def compute_img_mean_std(image_paths):\n","    \"\"\"\n","        computing the mean and std of three channel on the whole dataset,\n","        first we should normalize the image from 0-255 to 0-1\n","    \"\"\"\n","    print(image_paths)\n","    img_h, img_w = 224, 224\n","    imgs = []\n","    means, stdevs = [], []\n","\n","    print(len(image_paths))\n","\n","    for i in tqdm(range(len(image_paths))):\n","        print(image_paths[i])\n","        img = cv2.imread(image_paths[i])\n","        print(img)\n","        imgs.append(img)\n","    \n","\n","    imgs = np.stack(imgs, axis=3)\n","    print(imgs.shape)\n","\n","    imgs = imgs.astype(np.float32) / 255.\n","\n","    for i in range(3):\n","        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n","        means.append(np.mean(pixels))\n","        stdevs.append(np.std(pixels))\n","\n","    means.reverse()  # BGR --> RGB\n","    stdevs.reverse()\n","\n","    print(\"normMean = {}\".format(means))\n","    print(\"normStd = {}\".format(stdevs))\n","    return means,stdevs\n","\n","means, stdevs = compute_img_mean_std('/content/drive/My Drive/Master/TFM/224/akiek')\n","\n","print(means)\n","print(stdevs)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOBioycN91544kjK8IKGAfX","collapsed_sections":[],"mount_file_id":"1vaAbyPKQGUcqTYlL-GIhxANRjMXgOhbG","name":"Estudio Dataset.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
